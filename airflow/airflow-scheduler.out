[[34m2024-06-16T02:03:52.339-0300[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-06-16T02:03:52.342-0300[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2024-06-16T02:03:52.423-0300[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-06-16T02:03:52.423-0300[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-06-16T02:03:52.433-0300[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 26071[0m
[[34m2024-06-16T02:03:52.434-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T02:03:52.438-0300[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-06-16T02:03:52.458-0300] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-06-16T02:03:52.462-0300[0m] {[34mscheduler_job_runner.py:[0m1618} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2024-06-16T02:08:52.670-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T02:13:52.854-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T02:18:53.015-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T02:23:53.192-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T02:28:53.303-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T02:33:53.409-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T02:38:53.608-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T02:43:53.898-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T02:44:29.378-0300[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for import_github_data_to_postgres to 2024-06-16 00:00:00+00:00, run_after=2024-06-17 00:00:00+00:00[0m
[[34m2024-06-16T02:44:29.556-0300[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github scheduled__2024-06-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T05:44:27.972947+00:00 [scheduled]>[0m
[[34m2024-06-16T02:44:29.557-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG import_github_data_to_postgres has 0/16 running and queued tasks[0m
[[34m2024-06-16T02:44:29.558-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG import_github_data_to_postgres has 1/16 running and queued tasks[0m
[[34m2024-06-16T02:44:29.558-0300[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github scheduled__2024-06-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T05:44:27.972947+00:00 [scheduled]>[0m
[[34m2024-06-16T02:44:29.581-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='scheduled__2024-06-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-16T02:44:29.581-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'scheduled__2024-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T02:44:29.583-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T05:44:27.972947+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-16T02:44:29.584-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T05:44:27.972947+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T02:44:29.593-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'scheduled__2024-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T02:44:30.926-0300[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/maira/airflow/dags/extract_data.py[0m
[[34m2024-06-16T02:44:31.514-0300[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T02:44:31.575-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/maira/.local/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-16T02:44:31.575-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T02:44:32.061-0300[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: import_github_data_to_postgres.download_file_from_github scheduled__2024-06-15T00:00:00+00:00 [queued]> on host LAPTOP-6NHM3R4R.[0m
[[34m2024-06-16T02:44:34.560-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T05:44:27.972947+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T02:44:35.502-0300[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/maira/airflow/dags/extract_data.py[0m
[[34m2024-06-16T02:44:35.885-0300[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T02:44:35.924-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/maira/.local/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-16T02:44:35.924-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T02:44:36.276-0300[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T05:44:27.972947+00:00 [queued]> on host LAPTOP-6NHM3R4R.[0m
[[34m2024-06-16T02:44:37.474-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='scheduled__2024-06-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-16T02:44:37.475-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T05:44:27.972947+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-16T02:44:37.487-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T05:44:27.972947+00:00, map_index=-1, run_start_date=2024-06-16 05:44:36.325226+00:00, run_end_date=2024-06-16 05:44:36.944843+00:00, run_duration=0.619617, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=9, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-06-16 05:44:29.562192+00:00, queued_by_job_id=7, pid=29443[0m
[[34m2024-06-16T02:44:37.488-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=scheduled__2024-06-15T00:00:00+00:00, map_index=-1, run_start_date=2024-06-16 05:44:32.183172+00:00, run_end_date=2024-06-16 05:44:34.018847+00:00, run_duration=1.835675, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=8, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-06-16 05:44:29.562192+00:00, queued_by_job_id=7, pid=29425[0m
[[34m2024-06-16T02:48:54.069-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T02:49:34.221-0300[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github scheduled__2024-06-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-06-16T02:49:34.221-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG import_github_data_to_postgres has 0/16 running and queued tasks[0m
[[34m2024-06-16T02:49:34.222-0300[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github scheduled__2024-06-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-06-16T02:49:34.229-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='scheduled__2024-06-15T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-16T02:49:34.230-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'scheduled__2024-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T02:49:34.237-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'scheduled__2024-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T02:49:35.369-0300[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/maira/airflow/dags/extract_data.py[0m
[[34m2024-06-16T02:49:35.927-0300[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T02:49:35.980-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/maira/.local/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-16T02:49:35.980-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T02:49:36.375-0300[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: import_github_data_to_postgres.download_file_from_github scheduled__2024-06-15T00:00:00+00:00 [queued]> on host LAPTOP-6NHM3R4R.[0m
[[34m2024-06-16T02:49:38.480-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='scheduled__2024-06-15T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-06-16T02:49:38.494-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=scheduled__2024-06-15T00:00:00+00:00, map_index=-1, run_start_date=2024-06-16 05:49:36.435455+00:00, run_end_date=2024-06-16 05:49:37.956640+00:00, run_duration=1.521185, state=failed, executor_state=success, try_number=2, max_tries=1, job_id=10, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-06-16 05:49:34.222832+00:00, queued_by_job_id=7, pid=30073[0m
[[34m2024-06-16T02:49:38.656-0300[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun import_github_data_to_postgres @ 2024-06-15 00:00:00+00:00: scheduled__2024-06-15T00:00:00+00:00, state:running, queued_at: 2024-06-16 05:44:29.228516+00:00. externally triggered: False> failed[0m
[[34m2024-06-16T02:49:38.665-0300[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=import_github_data_to_postgres, execution_date=2024-06-15 00:00:00+00:00, run_id=scheduled__2024-06-15T00:00:00+00:00, run_start_date=2024-06-16 05:44:29.407679+00:00, run_end_date=2024-06-16 05:49:38.664006+00:00, run_duration=309.256327, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-15 00:00:00+00:00, data_interval_end=2024-06-16 00:00:00+00:00, dag_hash=76f9a160572ff11e4a1a5c742f01b071[0m
[[34m2024-06-16T02:49:38.677-0300[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for import_github_data_to_postgres to 2024-06-16 00:00:00+00:00, run_after=2024-06-17 00:00:00+00:00[0m
[[34m2024-06-16T02:49:38.702-0300[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T05:44:27.972947+00:00 [scheduled]>[0m
[[34m2024-06-16T02:49:38.702-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG import_github_data_to_postgres has 0/16 running and queued tasks[0m
[[34m2024-06-16T02:49:38.702-0300[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T05:44:27.972947+00:00 [scheduled]>[0m
[[34m2024-06-16T02:49:38.704-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T05:44:27.972947+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-16T02:49:38.704-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T05:44:27.972947+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T02:49:38.713-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T05:44:27.972947+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T02:49:39.509-0300[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/maira/airflow/dags/extract_data.py[0m
[[34m2024-06-16T02:49:39.898-0300[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T02:49:39.938-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/maira/.local/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-16T02:49:39.939-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T02:49:40.248-0300[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T05:44:27.972947+00:00 [queued]> on host LAPTOP-6NHM3R4R.[0m
[[34m2024-06-16T02:49:40.734-0300[0m] {[34msequential_executor.py:[0m81} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T05:44:27.972947+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py']' returned non-zero exit status 1..[0m
[[34m2024-06-16T02:49:40.735-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T05:44:27.972947+00:00', try_number=2, map_index=-1)[0m
[[34m2024-06-16T02:49:40.739-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T05:44:27.972947+00:00, map_index=-1, run_start_date=2024-06-16 05:44:36.325226+00:00, run_end_date=2024-06-16 05:44:36.944843+00:00, run_duration=0.619617, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=9, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-06-16 05:49:38.703343+00:00, queued_by_job_id=7, pid=29443[0m
[[34m2024-06-16T02:49:40.740-0300[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T05:44:27.972947+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T02:49:40.784-0300[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T05:44:27.972947+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T02:49:40.813-0300[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T05:44:27.972947+00:00, execution_date=20240616T054427, start_date=20240616T054436, end_date=20240616T054940[0m
[[34m2024-06-16T02:49:42.144-0300[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun import_github_data_to_postgres @ 2024-06-16 05:44:27.972947+00:00: manual__2024-06-16T05:44:27.972947+00:00, state:running, queued_at: 2024-06-16 05:44:28.158002+00:00. externally triggered: True> failed[0m
[[34m2024-06-16T02:49:42.144-0300[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=import_github_data_to_postgres, execution_date=2024-06-16 05:44:27.972947+00:00, run_id=manual__2024-06-16T05:44:27.972947+00:00, run_start_date=2024-06-16 05:44:29.417288+00:00, run_end_date=2024-06-16 05:49:42.144568+00:00, run_duration=312.72728, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-06-15 00:00:00+00:00, data_interval_end=2024-06-16 00:00:00+00:00, dag_hash=76f9a160572ff11e4a1a5c742f01b071[0m
[2024-06-16T02:50:16.741-0300] {manager.py:524} INFO - DAG example_task_group is missing and will be deactivated.
[2024-06-16T02:50:16.742-0300] {manager.py:524} INFO - DAG tutorial_taskflow_api_virtualenv is missing and will be deactivated.
[2024-06-16T02:50:16.743-0300] {manager.py:524} INFO - DAG tutorial_dag is missing and will be deactivated.
[2024-06-16T02:50:16.743-0300] {manager.py:524} INFO - DAG example_trigger_target_dag is missing and will be deactivated.
[2024-06-16T02:50:16.743-0300] {manager.py:524} INFO - DAG example_skip_dag is missing and will be deactivated.
[2024-06-16T02:50:16.743-0300] {manager.py:524} INFO - DAG example_setup_teardown is missing and will be deactivated.
[2024-06-16T02:50:16.750-0300] {manager.py:536} INFO - Deactivated 6 DAGs which are no longer present in file.
[2024-06-16T02:50:16.774-0300] {manager.py:540} INFO - Deleted DAG example_skip_dag in serialized_dag table
[2024-06-16T02:50:16.786-0300] {manager.py:540} INFO - Deleted DAG example_trigger_target_dag in serialized_dag table
[2024-06-16T02:50:16.799-0300] {manager.py:540} INFO - Deleted DAG example_task_group in serialized_dag table
[2024-06-16T02:50:16.810-0300] {manager.py:540} INFO - Deleted DAG tutorial_taskflow_api_virtualenv in serialized_dag table
[2024-06-16T02:50:16.822-0300] {manager.py:540} INFO - Deleted DAG tutorial_dag in serialized_dag table
[2024-06-16T02:50:16.834-0300] {manager.py:540} INFO - Deleted DAG example_setup_teardown in serialized_dag table
[[34m2024-06-16T02:53:54.225-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T02:58:54.389-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T03:03:54.573-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T03:08:54.754-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T03:13:55.011-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T03:18:55.197-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T03:20:16.058-0300[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:20:14.078498+00:00 [scheduled]>[0m
[[34m2024-06-16T03:20:16.059-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG import_github_data_to_postgres has 0/16 running and queued tasks[0m
[[34m2024-06-16T03:20:16.060-0300[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:20:14.078498+00:00 [scheduled]>[0m
[[34m2024-06-16T03:20:16.067-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T06:20:14.078498+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-16T03:20:16.068-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:20:14.078498+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T03:20:16.076-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:20:14.078498+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T03:20:19.196-0300[0m] {[34mcli_parser.py:[0m77} WARNING[0m - cannot load CLI commands from auth manager: Type annotation for "TaskInstance.dag_model" can't be correctly interpreted for Annotated Declarative Table form.  ORM annotations should normally make use of the ``Mapped[]`` generic type, or other ORM-compatible generic type, as a container for the actual type, which indicates the intent that the attribute is mapped. Class variables that are not intended to be mapped by the ORM should use ClassVar[].  To allow Annotated Declarative to disregard legacy annotations which don't use Mapped[] to pass, set "__allow_unmapped__ = True" on the class or a superclass this class. (Background on this error at: https://sqlalche.me/e/20/zlpr)[0m
[[34m2024-06-16T03:20:19.199-0300[0m] {[34mcli_parser.py:[0m78} WARNING[0m - Authentication manager is not configured and webserver will not be able to start.[0m
[[34m2024-06-16T03:20:20.035-0300[0m] {[34msequential_executor.py:[0m81} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:20:14.078498+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py']' returned non-zero exit status 1..[0m
[[34m2024-06-16T03:20:20.037-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T06:20:14.078498+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-16T03:20:20.042-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T06:20:14.078498+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-06-16 06:20:16.061360+00:00, queued_by_job_id=7, pid=None[0m
[[34m2024-06-16T03:20:20.043-0300[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:20:14.078498+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T03:20:20.074-0300[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:20:14.078498+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T03:20:20.086-0300[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T06:20:14.078498+00:00, execution_date=20240616T062014, start_date=, end_date=20240616T062020[0m
[[34m2024-06-16T03:23:55.395-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T03:25:20.980-0300[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:20:14.078498+00:00 [scheduled]>[0m
[[34m2024-06-16T03:25:20.981-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG import_github_data_to_postgres has 0/16 running and queued tasks[0m
[[34m2024-06-16T03:25:20.981-0300[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:20:14.078498+00:00 [scheduled]>[0m
[[34m2024-06-16T03:25:20.991-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T06:20:14.078498+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-16T03:25:20.992-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:20:14.078498+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T03:25:21.001-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:20:14.078498+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T03:25:23.052-0300[0m] {[34mcli_parser.py:[0m77} WARNING[0m - cannot load CLI commands from auth manager: Type annotation for "TaskInstance.dag_model" can't be correctly interpreted for Annotated Declarative Table form.  ORM annotations should normally make use of the ``Mapped[]`` generic type, or other ORM-compatible generic type, as a container for the actual type, which indicates the intent that the attribute is mapped. Class variables that are not intended to be mapped by the ORM should use ClassVar[].  To allow Annotated Declarative to disregard legacy annotations which don't use Mapped[] to pass, set "__allow_unmapped__ = True" on the class or a superclass this class. (Background on this error at: https://sqlalche.me/e/20/zlpr)[0m
[[34m2024-06-16T03:25:23.054-0300[0m] {[34mcli_parser.py:[0m78} WARNING[0m - Authentication manager is not configured and webserver will not be able to start.[0m
[[34m2024-06-16T03:25:23.703-0300[0m] {[34msequential_executor.py:[0m81} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:20:14.078498+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py']' returned non-zero exit status 1..[0m
[[34m2024-06-16T03:25:23.704-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T06:20:14.078498+00:00', try_number=2, map_index=-1)[0m
[[34m2024-06-16T03:25:23.709-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T06:20:14.078498+00:00, map_index=-1, run_start_date=None, run_end_date=2024-06-16 06:20:20.079397+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-06-16 06:25:20.982518+00:00, queued_by_job_id=7, pid=None[0m
[[34m2024-06-16T03:25:23.709-0300[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:20:14.078498+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T03:25:23.746-0300[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:20:14.078498+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T03:25:23.765-0300[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T06:20:14.078498+00:00, execution_date=20240616T062014, start_date=, end_date=20240616T062523[0m
[[34m2024-06-16T03:25:25.111-0300[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun import_github_data_to_postgres @ 2024-06-16 06:20:14.078498+00:00: manual__2024-06-16T06:20:14.078498+00:00, state:running, queued_at: 2024-06-16 06:20:14.375130+00:00. externally triggered: True> failed[0m
[[34m2024-06-16T03:25:25.112-0300[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=import_github_data_to_postgres, execution_date=2024-06-16 06:20:14.078498+00:00, run_id=manual__2024-06-16T06:20:14.078498+00:00, run_start_date=2024-06-16 06:20:15.987615+00:00, run_end_date=2024-06-16 06:25:25.111402+00:00, run_duration=309.123787, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-06-15 00:00:00+00:00, data_interval_end=2024-06-16 00:00:00+00:00, dag_hash=76f9a160572ff11e4a1a5c742f01b071[0m
[[34m2024-06-16T03:27:32.948-0300[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:27:31.712331+00:00 [scheduled]>[0m
[[34m2024-06-16T03:27:32.956-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG import_github_data_to_postgres has 0/16 running and queued tasks[0m
[[34m2024-06-16T03:27:32.957-0300[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:27:31.712331+00:00 [scheduled]>[0m
[[34m2024-06-16T03:27:32.963-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T06:27:31.712331+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-16T03:27:32.964-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:27:31.712331+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T03:27:32.971-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:27:31.712331+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T03:27:34.547-0300[0m] {[34mcli_parser.py:[0m77} WARNING[0m - cannot load CLI commands from auth manager: Type annotation for "TaskInstance.dag_model" can't be correctly interpreted for Annotated Declarative Table form.  ORM annotations should normally make use of the ``Mapped[]`` generic type, or other ORM-compatible generic type, as a container for the actual type, which indicates the intent that the attribute is mapped. Class variables that are not intended to be mapped by the ORM should use ClassVar[].  To allow Annotated Declarative to disregard legacy annotations which don't use Mapped[] to pass, set "__allow_unmapped__ = True" on the class or a superclass this class. (Background on this error at: https://sqlalche.me/e/20/zlpr)[0m
[[34m2024-06-16T03:27:34.549-0300[0m] {[34mcli_parser.py:[0m78} WARNING[0m - Authentication manager is not configured and webserver will not be able to start.[0m
[[34m2024-06-16T03:27:35.080-0300[0m] {[34msequential_executor.py:[0m81} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:27:31.712331+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py']' returned non-zero exit status 1..[0m
[[34m2024-06-16T03:27:35.081-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T06:27:31.712331+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-16T03:27:35.085-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T06:27:31.712331+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-06-16 06:27:32.957811+00:00, queued_by_job_id=7, pid=None[0m
[[34m2024-06-16T03:27:35.086-0300[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:27:31.712331+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T03:27:35.120-0300[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:27:31.712331+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T03:27:35.134-0300[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T06:27:31.712331+00:00, execution_date=20240616T062731, start_date=, end_date=20240616T062735[0m
[[34m2024-06-16T03:28:55.591-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T03:32:36.118-0300[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:27:31.712331+00:00 [scheduled]>[0m
[[34m2024-06-16T03:32:36.118-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG import_github_data_to_postgres has 0/16 running and queued tasks[0m
[[34m2024-06-16T03:32:36.118-0300[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:27:31.712331+00:00 [scheduled]>[0m
[[34m2024-06-16T03:32:36.121-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T06:27:31.712331+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-16T03:32:36.122-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:27:31.712331+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T03:32:36.131-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:27:31.712331+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T03:32:37.141-0300[0m] {[34mcli_parser.py:[0m77} WARNING[0m - cannot load CLI commands from auth manager: Type annotation for "TaskInstance.dag_model" can't be correctly interpreted for Annotated Declarative Table form.  ORM annotations should normally make use of the ``Mapped[]`` generic type, or other ORM-compatible generic type, as a container for the actual type, which indicates the intent that the attribute is mapped. Class variables that are not intended to be mapped by the ORM should use ClassVar[].  To allow Annotated Declarative to disregard legacy annotations which don't use Mapped[] to pass, set "__allow_unmapped__ = True" on the class or a superclass this class. (Background on this error at: https://sqlalche.me/e/20/zlpr)[0m
[[34m2024-06-16T03:32:37.144-0300[0m] {[34mcli_parser.py:[0m78} WARNING[0m - Authentication manager is not configured and webserver will not be able to start.[0m
[[34m2024-06-16T03:32:37.718-0300[0m] {[34msequential_executor.py:[0m81} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:27:31.712331+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py']' returned non-zero exit status 1..[0m
[[34m2024-06-16T03:32:37.719-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T06:27:31.712331+00:00', try_number=2, map_index=-1)[0m
[[34m2024-06-16T03:32:37.722-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T06:27:31.712331+00:00, map_index=-1, run_start_date=None, run_end_date=2024-06-16 06:27:35.125449+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-06-16 06:32:36.118891+00:00, queued_by_job_id=7, pid=None[0m
[[34m2024-06-16T03:32:37.723-0300[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:27:31.712331+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T03:32:37.730-0300[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:27:31.712331+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T03:32:37.736-0300[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T06:27:31.712331+00:00, execution_date=20240616T062731, start_date=, end_date=20240616T063237[0m
[[34m2024-06-16T03:32:39.064-0300[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun import_github_data_to_postgres @ 2024-06-16 06:27:31.712331+00:00: manual__2024-06-16T06:27:31.712331+00:00, state:running, queued_at: 2024-06-16 06:27:31.877463+00:00. externally triggered: True> failed[0m
[[34m2024-06-16T03:32:39.066-0300[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=import_github_data_to_postgres, execution_date=2024-06-16 06:27:31.712331+00:00, run_id=manual__2024-06-16T06:27:31.712331+00:00, run_start_date=2024-06-16 06:27:32.871992+00:00, run_end_date=2024-06-16 06:32:39.065405+00:00, run_duration=306.193413, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-06-15 00:00:00+00:00, data_interval_end=2024-06-16 00:00:00+00:00, dag_hash=76f9a160572ff11e4a1a5c742f01b071[0m
[[34m2024-06-16T03:33:55.747-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T03:38:55.897-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T03:41:29.538-0300[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:41:28.222703+00:00 [scheduled]>[0m
[[34m2024-06-16T03:41:29.546-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG import_github_data_to_postgres has 0/16 running and queued tasks[0m
[[34m2024-06-16T03:41:29.547-0300[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:41:28.222703+00:00 [scheduled]>[0m
[[34m2024-06-16T03:41:29.558-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T06:41:28.222703+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-16T03:41:29.559-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:41:28.222703+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T03:41:29.569-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:41:28.222703+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T03:41:32.121-0300[0m] {[34mcli_parser.py:[0m77} WARNING[0m - cannot load CLI commands from auth manager: Type annotation for "TaskInstance.dag_model" can't be correctly interpreted for Annotated Declarative Table form.  ORM annotations should normally make use of the ``Mapped[]`` generic type, or other ORM-compatible generic type, as a container for the actual type, which indicates the intent that the attribute is mapped. Class variables that are not intended to be mapped by the ORM should use ClassVar[].  To allow Annotated Declarative to disregard legacy annotations which don't use Mapped[] to pass, set "__allow_unmapped__ = True" on the class or a superclass this class. (Background on this error at: https://sqlalche.me/e/20/zlpr)[0m
[[34m2024-06-16T03:41:32.123-0300[0m] {[34mcli_parser.py:[0m78} WARNING[0m - Authentication manager is not configured and webserver will not be able to start.[0m
[[34m2024-06-16T03:41:32.662-0300[0m] {[34msequential_executor.py:[0m81} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:41:28.222703+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py']' returned non-zero exit status 1..[0m
[[34m2024-06-16T03:41:32.663-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T06:41:28.222703+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-16T03:41:32.667-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T06:41:28.222703+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-06-16 06:41:29.548802+00:00, queued_by_job_id=7, pid=None[0m
[[34m2024-06-16T03:41:32.668-0300[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:41:28.222703+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T03:41:32.714-0300[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:41:28.222703+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T03:41:32.733-0300[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T06:41:28.222703+00:00, execution_date=20240616T064128, start_date=, end_date=20240616T064132[0m
[[34m2024-06-16T03:43:55.971-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T03:46:33.634-0300[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:41:28.222703+00:00 [scheduled]>[0m
[[34m2024-06-16T03:46:33.634-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG import_github_data_to_postgres has 0/16 running and queued tasks[0m
[[34m2024-06-16T03:46:33.634-0300[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:41:28.222703+00:00 [scheduled]>[0m
[[34m2024-06-16T03:46:33.640-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T06:41:28.222703+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-16T03:46:33.641-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:41:28.222703+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T03:46:33.649-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:41:28.222703+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T03:46:35.170-0300[0m] {[34mcli_parser.py:[0m77} WARNING[0m - cannot load CLI commands from auth manager: Type annotation for "TaskInstance.dag_model" can't be correctly interpreted for Annotated Declarative Table form.  ORM annotations should normally make use of the ``Mapped[]`` generic type, or other ORM-compatible generic type, as a container for the actual type, which indicates the intent that the attribute is mapped. Class variables that are not intended to be mapped by the ORM should use ClassVar[].  To allow Annotated Declarative to disregard legacy annotations which don't use Mapped[] to pass, set "__allow_unmapped__ = True" on the class or a superclass this class. (Background on this error at: https://sqlalche.me/e/20/zlpr)[0m
[[34m2024-06-16T03:46:35.173-0300[0m] {[34mcli_parser.py:[0m78} WARNING[0m - Authentication manager is not configured and webserver will not be able to start.[0m
[[34m2024-06-16T03:46:35.646-0300[0m] {[34msequential_executor.py:[0m81} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:41:28.222703+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py']' returned non-zero exit status 1..[0m
[[34m2024-06-16T03:46:35.647-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T06:41:28.222703+00:00', try_number=2, map_index=-1)[0m
[[34m2024-06-16T03:46:35.652-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T06:41:28.222703+00:00, map_index=-1, run_start_date=None, run_end_date=2024-06-16 06:41:32.720901+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-06-16 06:46:33.635501+00:00, queued_by_job_id=7, pid=None[0m
[[34m2024-06-16T03:46:35.653-0300[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:41:28.222703+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T03:46:35.668-0300[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:41:28.222703+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T03:46:35.678-0300[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T06:41:28.222703+00:00, execution_date=20240616T064128, start_date=, end_date=20240616T064635[0m
[[34m2024-06-16T03:46:36.355-0300[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun import_github_data_to_postgres @ 2024-06-16 06:41:28.222703+00:00: manual__2024-06-16T06:41:28.222703+00:00, state:running, queued_at: 2024-06-16 06:41:28.387037+00:00. externally triggered: True> failed[0m
[[34m2024-06-16T03:46:36.356-0300[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=import_github_data_to_postgres, execution_date=2024-06-16 06:41:28.222703+00:00, run_id=manual__2024-06-16T06:41:28.222703+00:00, run_start_date=2024-06-16 06:41:29.461257+00:00, run_end_date=2024-06-16 06:46:36.356065+00:00, run_duration=306.894808, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-06-15 00:00:00+00:00, data_interval_end=2024-06-16 00:00:00+00:00, dag_hash=76f9a160572ff11e4a1a5c742f01b071[0m
[[34m2024-06-16T03:48:56.057-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T03:51:39.358-0300[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:51:37.135803+00:00 [scheduled]>[0m
[[34m2024-06-16T03:51:39.368-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG import_github_data_to_postgres has 0/16 running and queued tasks[0m
[[34m2024-06-16T03:51:39.368-0300[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:51:37.135803+00:00 [scheduled]>[0m
[[34m2024-06-16T03:51:39.388-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T06:51:37.135803+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-16T03:51:39.389-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:51:37.135803+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T03:51:39.398-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:51:37.135803+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T03:51:44.042-0300[0m] {[34mcli_parser.py:[0m77} WARNING[0m - cannot load CLI commands from auth manager: Type annotation for "TaskInstance.dag_model" can't be correctly interpreted for Annotated Declarative Table form.  ORM annotations should normally make use of the ``Mapped[]`` generic type, or other ORM-compatible generic type, as a container for the actual type, which indicates the intent that the attribute is mapped. Class variables that are not intended to be mapped by the ORM should use ClassVar[].  To allow Annotated Declarative to disregard legacy annotations which don't use Mapped[] to pass, set "__allow_unmapped__ = True" on the class or a superclass this class. (Background on this error at: https://sqlalche.me/e/20/zlpr)[0m
[[34m2024-06-16T03:51:44.044-0300[0m] {[34mcli_parser.py:[0m78} WARNING[0m - Authentication manager is not configured and webserver will not be able to start.[0m
[[34m2024-06-16T03:51:44.786-0300[0m] {[34msequential_executor.py:[0m81} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:51:37.135803+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py']' returned non-zero exit status 1..[0m
[[34m2024-06-16T03:51:44.788-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T06:51:37.135803+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-16T03:51:44.797-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T06:51:37.135803+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-06-16 06:51:39.369630+00:00, queued_by_job_id=7, pid=None[0m
[[34m2024-06-16T03:51:44.798-0300[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:51:37.135803+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T03:51:44.873-0300[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:51:37.135803+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T03:51:44.911-0300[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T06:51:37.135803+00:00, execution_date=20240616T065137, start_date=, end_date=20240616T065144[0m
[[34m2024-06-16T03:53:56.241-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T03:56:45.439-0300[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:51:37.135803+00:00 [scheduled]>[0m
[[34m2024-06-16T03:56:45.439-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG import_github_data_to_postgres has 0/16 running and queued tasks[0m
[[34m2024-06-16T03:56:45.439-0300[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:51:37.135803+00:00 [scheduled]>[0m
[[34m2024-06-16T03:56:45.443-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T06:51:37.135803+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-16T03:56:45.443-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:51:37.135803+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T03:56:45.452-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:51:37.135803+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T03:56:46.432-0300[0m] {[34mcli_parser.py:[0m77} WARNING[0m - cannot load CLI commands from auth manager: Type annotation for "TaskInstance.dag_model" can't be correctly interpreted for Annotated Declarative Table form.  ORM annotations should normally make use of the ``Mapped[]`` generic type, or other ORM-compatible generic type, as a container for the actual type, which indicates the intent that the attribute is mapped. Class variables that are not intended to be mapped by the ORM should use ClassVar[].  To allow Annotated Declarative to disregard legacy annotations which don't use Mapped[] to pass, set "__allow_unmapped__ = True" on the class or a superclass this class. (Background on this error at: https://sqlalche.me/e/20/zlpr)[0m
[[34m2024-06-16T03:56:46.433-0300[0m] {[34mcli_parser.py:[0m78} WARNING[0m - Authentication manager is not configured and webserver will not be able to start.[0m
[[34m2024-06-16T03:56:46.897-0300[0m] {[34msequential_executor.py:[0m81} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T06:51:37.135803+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py']' returned non-zero exit status 1..[0m
[[34m2024-06-16T03:56:46.899-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T06:51:37.135803+00:00', try_number=2, map_index=-1)[0m
[[34m2024-06-16T03:56:46.902-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T06:51:37.135803+00:00, map_index=-1, run_start_date=None, run_end_date=2024-06-16 06:51:44.884566+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-06-16 06:56:45.440169+00:00, queued_by_job_id=7, pid=None[0m
[[34m2024-06-16T03:56:46.903-0300[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:51:37.135803+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T03:56:46.922-0300[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T06:51:37.135803+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T03:56:46.934-0300[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T06:51:37.135803+00:00, execution_date=20240616T065137, start_date=, end_date=20240616T065646[0m
[[34m2024-06-16T03:56:48.271-0300[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun import_github_data_to_postgres @ 2024-06-16 06:51:37.135803+00:00: manual__2024-06-16T06:51:37.135803+00:00, state:running, queued_at: 2024-06-16 06:51:37.279222+00:00. externally triggered: True> failed[0m
[[34m2024-06-16T03:56:48.272-0300[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=import_github_data_to_postgres, execution_date=2024-06-16 06:51:37.135803+00:00, run_id=manual__2024-06-16T06:51:37.135803+00:00, run_start_date=2024-06-16 06:51:39.083357+00:00, run_end_date=2024-06-16 06:56:48.272154+00:00, run_duration=309.188797, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-06-15 00:00:00+00:00, data_interval_end=2024-06-16 00:00:00+00:00, dag_hash=76f9a160572ff11e4a1a5c742f01b071[0m
[[34m2024-06-16T03:58:56.397-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T04:00:54.443-0300[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T07:00:53.316903+00:00 [scheduled]>[0m
[[34m2024-06-16T04:00:54.444-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG import_github_data_to_postgres has 0/16 running and queued tasks[0m
[[34m2024-06-16T04:00:54.444-0300[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T07:00:53.316903+00:00 [scheduled]>[0m
[[34m2024-06-16T04:00:54.447-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T07:00:53.316903+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-16T04:00:54.447-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T07:00:53.316903+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T04:00:54.456-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T07:00:53.316903+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T04:00:56.174-0300[0m] {[34mcli_parser.py:[0m77} WARNING[0m - cannot load CLI commands from auth manager: Type annotation for "TaskInstance.dag_model" can't be correctly interpreted for Annotated Declarative Table form.  ORM annotations should normally make use of the ``Mapped[]`` generic type, or other ORM-compatible generic type, as a container for the actual type, which indicates the intent that the attribute is mapped. Class variables that are not intended to be mapped by the ORM should use ClassVar[].  To allow Annotated Declarative to disregard legacy annotations which don't use Mapped[] to pass, set "__allow_unmapped__ = True" on the class or a superclass this class. (Background on this error at: https://sqlalche.me/e/20/zlpr)[0m
[[34m2024-06-16T04:00:56.176-0300[0m] {[34mcli_parser.py:[0m78} WARNING[0m - Authentication manager is not configured and webserver will not be able to start.[0m
[[34m2024-06-16T04:00:56.862-0300[0m] {[34msequential_executor.py:[0m81} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'import_github_data_to_postgres', 'download_file_from_github', 'manual__2024-06-16T07:00:53.316903+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py']' returned non-zero exit status 1..[0m
[[34m2024-06-16T04:00:56.863-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='import_github_data_to_postgres', task_id='download_file_from_github', run_id='manual__2024-06-16T07:00:53.316903+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-16T04:00:56.868-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T07:00:53.316903+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-06-16 07:00:54.445109+00:00, queued_by_job_id=7, pid=None[0m
[[34m2024-06-16T04:00:56.869-0300[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T07:00:53.316903+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T04:00:56.902-0300[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - Executor reports task instance <TaskInstance: import_github_data_to_postgres.download_file_from_github manual__2024-06-16T07:00:53.316903+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T04:00:56.917-0300[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=import_github_data_to_postgres, task_id=download_file_from_github, run_id=manual__2024-06-16T07:00:53.316903+00:00, execution_date=20240616T070053, start_date=, end_date=20240616T070056[0m
[2024-06-16T04:01:00.919-0300] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.docker.operators.docker' found in /home/maira/airflow/dags/extract_data.py: No module named 'airflow.providers.docker'
[2024-06-16T04:01:50.867-0300] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.docker.operators.docker' found in /home/maira/airflow/dags/extract_data.py: No module named 'airflow.providers.docker'
[2024-06-16T04:01:57.217-0300] {manager.py:524} INFO - DAG import_github_data_to_postgres is missing and will be deactivated.
[2024-06-16T04:01:57.219-0300] {manager.py:536} INFO - Deactivated 1 DAGs which are no longer present in file.
[2024-06-16T04:01:57.229-0300] {manager.py:540} INFO - Deleted DAG import_github_data_to_postgres in serialized_dag table
[2024-06-16T04:02:42.325-0300] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.docker.operators.docker' found in /home/maira/airflow/dags/extract_data.py: No module named 'airflow.providers.docker'
[[34m2024-06-16T04:03:56.557-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2024-06-16T04:05:59.264-0300] {manager.py:524} INFO - DAG lighthouse_challenge is missing and will be deactivated.
[2024-06-16T04:05:59.266-0300] {manager.py:536} INFO - Deactivated 1 DAGs which are no longer present in file.
[2024-06-16T04:05:59.279-0300] {manager.py:540} INFO - Deleted DAG lighthouse_challenge in serialized_dag table
[[34m2024-06-16T04:08:56.690-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T04:11:23.466-0300[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for indicium_challenge to 2024-06-16 00:00:00+00:00, run_after=2024-06-17 00:00:00+00:00[0m
[[34m2024-06-16T04:11:23.754-0300[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: indicium_challenge.extracts.csv scheduled__2024-06-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: indicium_challenge.extracts.csv manual__2024-06-16T07:11:21.759222+00:00 [scheduled]>[0m
[[34m2024-06-16T04:11:23.755-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG indicium_challenge has 0/16 running and queued tasks[0m
[[34m2024-06-16T04:11:23.755-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG indicium_challenge has 1/16 running and queued tasks[0m
[[34m2024-06-16T04:11:23.755-0300[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: indicium_challenge.extracts.csv scheduled__2024-06-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: indicium_challenge.extracts.csv manual__2024-06-16T07:11:21.759222+00:00 [scheduled]>[0m
[[34m2024-06-16T04:11:23.770-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='indicium_challenge', task_id='extracts.csv', run_id='scheduled__2024-06-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-06-16T04:11:23.772-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'indicium_challenge', 'extracts.csv', 'scheduled__2024-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T04:11:23.774-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='indicium_challenge', task_id='extracts.csv', run_id='manual__2024-06-16T07:11:21.759222+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-06-16T04:11:23.774-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'indicium_challenge', 'extracts.csv', 'manual__2024-06-16T07:11:21.759222+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T04:11:23.905-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'indicium_challenge', 'extracts.csv', 'scheduled__2024-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T04:11:29.680-0300[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/maira/airflow/dags/extract_data.py[0m
[[34m2024-06-16T04:11:30.294-0300[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T04:11:30.386-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/maira/.local/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-16T04:11:30.387-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T04:11:31.985-0300[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: indicium_challenge.extracts.csv scheduled__2024-06-15T00:00:00+00:00 [queued]> on host LAPTOP-6NHM3R4R.[0m
[[34m2024-06-16T04:11:33.381-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'indicium_challenge', 'extracts.csv', 'manual__2024-06-16T07:11:21.759222+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T04:11:34.477-0300[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/maira/airflow/dags/extract_data.py[0m
[[34m2024-06-16T04:11:34.681-0300[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T04:11:34.733-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/maira/.local/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-16T04:11:34.734-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T04:11:35.320-0300[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: indicium_challenge.extracts.csv manual__2024-06-16T07:11:21.759222+00:00 [queued]> on host LAPTOP-6NHM3R4R.[0m
[[34m2024-06-16T04:11:36.351-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='indicium_challenge', task_id='extracts.csv', run_id='scheduled__2024-06-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-16T04:11:36.351-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='indicium_challenge', task_id='extracts.csv', run_id='manual__2024-06-16T07:11:21.759222+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-16T04:11:36.360-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=indicium_challenge, task_id=extracts.csv, run_id=manual__2024-06-16T07:11:21.759222+00:00, map_index=-1, run_start_date=2024-06-16 07:11:35.371867+00:00, run_end_date=2024-06-16 07:11:35.686947+00:00, run_duration=0.31508, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=12, pool=default_pool, queue=default, priority_weight=3, operator=DockerOperator, queued_dttm=2024-06-16 07:11:23.757459+00:00, queued_by_job_id=7, pid=38289[0m
[[34m2024-06-16T04:11:36.361-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=indicium_challenge, task_id=extracts.csv, run_id=scheduled__2024-06-15T00:00:00+00:00, map_index=-1, run_start_date=2024-06-16 07:11:32.171787+00:00, run_end_date=2024-06-16 07:11:32.736207+00:00, run_duration=0.56442, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=11, pool=default_pool, queue=default, priority_weight=3, operator=DockerOperator, queued_dttm=2024-06-16 07:11:23.757459+00:00, queued_by_job_id=7, pid=38272[0m
[[34m2024-06-16T04:11:37.802-0300[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun indicium_challenge @ 2024-06-15 00:00:00+00:00: scheduled__2024-06-15T00:00:00+00:00, state:running, queued_at: 2024-06-16 07:11:23.352258+00:00. externally triggered: False> failed[0m
[[34m2024-06-16T04:11:37.803-0300[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=indicium_challenge, execution_date=2024-06-15 00:00:00+00:00, run_id=scheduled__2024-06-15T00:00:00+00:00, run_start_date=2024-06-16 07:11:23.637210+00:00, run_end_date=2024-06-16 07:11:37.803668+00:00, run_duration=14.166458, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-15 00:00:00+00:00, data_interval_end=2024-06-16 00:00:00+00:00, dag_hash=d01c8a25d62edc7e403ec12418bf6978[0m
[[34m2024-06-16T04:11:37.810-0300[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for indicium_challenge to 2024-06-16 00:00:00+00:00, run_after=2024-06-17 00:00:00+00:00[0m
[[34m2024-06-16T04:11:37.813-0300[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun indicium_challenge @ 2024-06-16 07:11:21.759222+00:00: manual__2024-06-16T07:11:21.759222+00:00, state:running, queued_at: 2024-06-16 07:11:21.913112+00:00. externally triggered: True> failed[0m
[[34m2024-06-16T04:11:37.814-0300[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=indicium_challenge, execution_date=2024-06-16 07:11:21.759222+00:00, run_id=manual__2024-06-16T07:11:21.759222+00:00, run_start_date=2024-06-16 07:11:23.646353+00:00, run_end_date=2024-06-16 07:11:37.814224+00:00, run_duration=14.167871, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-06-15 00:00:00+00:00, data_interval_end=2024-06-16 00:00:00+00:00, dag_hash=d01c8a25d62edc7e403ec12418bf6978[0m
[[34m2024-06-16T04:13:56.868-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T04:16:50.183-0300[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: indicium_challenge.extracts.csv manual__2024-06-16T07:16:48.576976+00:00 [scheduled]>[0m
[[34m2024-06-16T04:16:50.192-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG indicium_challenge has 0/16 running and queued tasks[0m
[[34m2024-06-16T04:16:50.193-0300[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: indicium_challenge.extracts.csv manual__2024-06-16T07:16:48.576976+00:00 [scheduled]>[0m
[[34m2024-06-16T04:16:50.218-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='indicium_challenge', task_id='extracts.csv', run_id='manual__2024-06-16T07:16:48.576976+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-06-16T04:16:50.220-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'indicium_challenge', 'extracts.csv', 'manual__2024-06-16T07:16:48.576976+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T04:16:50.236-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'indicium_challenge', 'extracts.csv', 'manual__2024-06-16T07:16:48.576976+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T04:16:53.213-0300[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/maira/airflow/dags/extract_data.py[0m
[[34m2024-06-16T04:16:53.430-0300[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T04:16:53.473-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/maira/.local/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-16T04:16:53.473-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T04:16:57.998-0300[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: indicium_challenge.extracts.csv manual__2024-06-16T07:16:48.576976+00:00 [queued]> on host LAPTOP-6NHM3R4R.[0m
[[34m2024-06-16T04:16:58.853-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='indicium_challenge', task_id='extracts.csv', run_id='manual__2024-06-16T07:16:48.576976+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-16T04:16:58.861-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=indicium_challenge, task_id=extracts.csv, run_id=manual__2024-06-16T07:16:48.576976+00:00, map_index=-1, run_start_date=2024-06-16 07:16:58.083810+00:00, run_end_date=2024-06-16 07:16:58.300049+00:00, run_duration=0.216239, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=13, pool=default_pool, queue=default, priority_weight=3, operator=DockerOperator, queued_dttm=2024-06-16 07:16:50.194207+00:00, queued_by_job_id=7, pid=38756[0m
[[34m2024-06-16T04:17:00.195-0300[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun indicium_challenge @ 2024-06-16 07:16:48.576976+00:00: manual__2024-06-16T07:16:48.576976+00:00, state:running, queued_at: 2024-06-16 07:16:48.896660+00:00. externally triggered: True> failed[0m
[[34m2024-06-16T04:17:00.195-0300[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=indicium_challenge, execution_date=2024-06-16 07:16:48.576976+00:00, run_id=manual__2024-06-16T07:16:48.576976+00:00, run_start_date=2024-06-16 07:16:50.059383+00:00, run_end_date=2024-06-16 07:17:00.195590+00:00, run_duration=10.136207, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-06-15 00:00:00+00:00, data_interval_end=2024-06-16 00:00:00+00:00, dag_hash=1e7b2027891a344c7b45d8f022ca3e8d[0m
[[34m2024-06-16T04:18:56.986-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T04:20:39.719-0300[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: indicium_challenge.extracts.csv manual__2024-06-16T07:20:38.230419+00:00 [scheduled]>[0m
[[34m2024-06-16T04:20:39.719-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG indicium_challenge has 0/16 running and queued tasks[0m
[[34m2024-06-16T04:20:39.719-0300[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: indicium_challenge.extracts.csv manual__2024-06-16T07:20:38.230419+00:00 [scheduled]>[0m
[[34m2024-06-16T04:20:39.726-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='indicium_challenge', task_id='extracts.csv', run_id='manual__2024-06-16T07:20:38.230419+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-06-16T04:20:39.727-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'indicium_challenge', 'extracts.csv', 'manual__2024-06-16T07:20:38.230419+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T04:20:39.736-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'indicium_challenge', 'extracts.csv', 'manual__2024-06-16T07:20:38.230419+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T04:20:42.219-0300[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/maira/airflow/dags/extract_data.py[0m
[[34m2024-06-16T04:20:42.417-0300[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T04:20:42.477-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/maira/.local/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-16T04:20:42.477-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T04:20:43.714-0300[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: indicium_challenge.extracts.csv manual__2024-06-16T07:20:38.230419+00:00 [queued]> on host LAPTOP-6NHM3R4R.[0m
[[34m2024-06-16T04:20:44.886-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='indicium_challenge', task_id='extracts.csv', run_id='manual__2024-06-16T07:20:38.230419+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-16T04:20:44.891-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=indicium_challenge, task_id=extracts.csv, run_id=manual__2024-06-16T07:20:38.230419+00:00, map_index=-1, run_start_date=2024-06-16 07:20:43.784581+00:00, run_end_date=2024-06-16 07:20:44.221059+00:00, run_duration=0.436478, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=14, pool=default_pool, queue=default, priority_weight=4, operator=DockerOperator, queued_dttm=2024-06-16 07:20:39.720398+00:00, queued_by_job_id=7, pid=39096[0m
[[34m2024-06-16T04:20:47.285-0300[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun indicium_challenge @ 2024-06-16 07:20:38.230419+00:00: manual__2024-06-16T07:20:38.230419+00:00, state:running, queued_at: 2024-06-16 07:20:38.477497+00:00. externally triggered: True> failed[0m
[[34m2024-06-16T04:20:47.286-0300[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=indicium_challenge, execution_date=2024-06-16 07:20:38.230419+00:00, run_id=manual__2024-06-16T07:20:38.230419+00:00, run_start_date=2024-06-16 07:20:39.639208+00:00, run_end_date=2024-06-16 07:20:47.286450+00:00, run_duration=7.647242, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-06-15 00:00:00+00:00, data_interval_end=2024-06-16 00:00:00+00:00, dag_hash=a754d83c50dc81b0f7ce9a0c525721d8[0m
[[34m2024-06-16T04:23:57.250-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2024-06-16T04:25:11.484-0300] {manager.py:524} INFO - DAG indicium_challenge is missing and will be deactivated.
[2024-06-16T04:25:11.489-0300] {manager.py:536} INFO - Deactivated 1 DAGs which are no longer present in file.
[2024-06-16T04:25:11.501-0300] {manager.py:540} INFO - Deleted DAG indicium_challenge in serialized_dag table
[[34m2024-06-16T04:27:26.629-0300[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: indicium_challenge.extracts.csv manual__2024-06-16T07:27:24.929763+00:00 [scheduled]>
	<TaskInstance: indicium_challenge.load_to_postgres manual__2024-06-16T07:27:24.929763+00:00 [scheduled]>[0m
[[34m2024-06-16T04:27:26.641-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG indicium_challenge has 0/16 running and queued tasks[0m
[[34m2024-06-16T04:27:26.642-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG indicium_challenge has 1/16 running and queued tasks[0m
[[34m2024-06-16T04:27:26.642-0300[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: indicium_challenge.extracts.csv manual__2024-06-16T07:27:24.929763+00:00 [scheduled]>
	<TaskInstance: indicium_challenge.load_to_postgres manual__2024-06-16T07:27:24.929763+00:00 [scheduled]>[0m
[[34m2024-06-16T04:27:26.655-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='indicium_challenge', task_id='extracts.csv', run_id='manual__2024-06-16T07:27:24.929763+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-06-16T04:27:26.655-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'indicium_challenge', 'extracts.csv', 'manual__2024-06-16T07:27:24.929763+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T04:27:26.656-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='indicium_challenge', task_id='load_to_postgres', run_id='manual__2024-06-16T07:27:24.929763+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-06-16T04:27:26.656-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'indicium_challenge', 'load_to_postgres', 'manual__2024-06-16T07:27:24.929763+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T04:27:26.665-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'indicium_challenge', 'extracts.csv', 'manual__2024-06-16T07:27:24.929763+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T04:27:29.832-0300[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/maira/airflow/dags/extract_data.py[0m
[[34m2024-06-16T04:27:30.096-0300[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T04:27:30.179-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/maira/.local/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-16T04:27:30.180-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T04:27:32.894-0300[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: indicium_challenge.extracts.csv manual__2024-06-16T07:27:24.929763+00:00 [queued]> on host LAPTOP-6NHM3R4R.[0m
[[34m2024-06-16T04:27:34.114-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'indicium_challenge', 'load_to_postgres', 'manual__2024-06-16T07:27:24.929763+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T04:27:35.282-0300[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/maira/airflow/dags/extract_data.py[0m
[[34m2024-06-16T04:27:35.445-0300[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T04:27:35.513-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/maira/.local/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-16T04:27:35.513-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T04:27:36.725-0300[0m] {[34msequential_executor.py:[0m81} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'indicium_challenge', 'load_to_postgres', 'manual__2024-06-16T07:27:24.929763+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py']' returned non-zero exit status 1..[0m
[[34m2024-06-16T04:27:36.729-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='indicium_challenge', task_id='extracts.csv', run_id='manual__2024-06-16T07:27:24.929763+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-16T04:27:36.730-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='indicium_challenge', task_id='load_to_postgres', run_id='manual__2024-06-16T07:27:24.929763+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-16T04:27:36.734-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=indicium_challenge, task_id=extracts.csv, run_id=manual__2024-06-16T07:27:24.929763+00:00, map_index=-1, run_start_date=2024-06-16 07:27:32.973234+00:00, run_end_date=2024-06-16 07:27:33.517296+00:00, run_duration=0.544062, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=15, pool=default_pool, queue=default, priority_weight=4, operator=DockerOperator, queued_dttm=2024-06-16 07:27:26.644098+00:00, queued_by_job_id=7, pid=39748[0m
[[34m2024-06-16T04:27:36.735-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=indicium_challenge, task_id=load_to_postgres, run_id=manual__2024-06-16T07:27:24.929763+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=removed, executor_state=failed, try_number=1, max_tries=0, job_id=None, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-06-16 07:27:26.644098+00:00, queued_by_job_id=7, pid=None[0m
[[34m2024-06-16T04:27:37.007-0300[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun indicium_challenge @ 2024-06-16 07:27:24.929763+00:00: manual__2024-06-16T07:27:24.929763+00:00, state:running, queued_at: 2024-06-16 07:27:25.193222+00:00. externally triggered: True> successful[0m
[[34m2024-06-16T04:27:37.008-0300[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=indicium_challenge, execution_date=2024-06-16 07:27:24.929763+00:00, run_id=manual__2024-06-16T07:27:24.929763+00:00, run_start_date=2024-06-16 07:27:26.490804+00:00, run_end_date=2024-06-16 07:27:37.007500+00:00, run_duration=10.516696, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-06-15 00:00:00+00:00, data_interval_end=2024-06-16 00:00:00+00:00, dag_hash=322e6d58357532c26a5904631cdbd008[0m
[[34m2024-06-16T04:28:57.389-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T10:25:01.866-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T10:30:14.999-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T10:35:18.381-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T10:38:34.617-0300[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: indicium_challenge.extracts.csv manual__2024-06-16T13:38:33.229324+00:00 [scheduled]>[0m
[[34m2024-06-16T10:38:34.632-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG indicium_challenge has 0/16 running and queued tasks[0m
[[34m2024-06-16T10:38:34.632-0300[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: indicium_challenge.extracts.csv manual__2024-06-16T13:38:33.229324+00:00 [scheduled]>[0m
[[34m2024-06-16T10:38:34.645-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='indicium_challenge', task_id='extracts.csv', run_id='manual__2024-06-16T13:38:33.229324+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-06-16T10:38:34.646-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'indicium_challenge', 'extracts.csv', 'manual__2024-06-16T13:38:33.229324+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T10:38:34.656-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'indicium_challenge', 'extracts.csv', 'manual__2024-06-16T13:38:33.229324+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T10:38:39.412-0300[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/maira/airflow/dags/extract_data.py[0m
[[34m2024-06-16T10:38:39.831-0300[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T10:38:39.905-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/maira/.local/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-16T10:38:39.907-0300[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-16T10:38:42.312-0300[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: indicium_challenge.extracts.csv manual__2024-06-16T13:38:33.229324+00:00 [queued]> on host LAPTOP-6NHM3R4R.[0m
[[34m2024-06-16T10:38:43.921-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='indicium_challenge', task_id='extracts.csv', run_id='manual__2024-06-16T13:38:33.229324+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-16T10:38:44.104-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=indicium_challenge, task_id=extracts.csv, run_id=manual__2024-06-16T13:38:33.229324+00:00, map_index=-1, run_start_date=2024-06-16 13:38:42.411117+00:00, run_end_date=2024-06-16 13:38:43.144316+00:00, run_duration=0.733199, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=16, pool=default_pool, queue=default, priority_weight=3, operator=DockerOperator, queued_dttm=2024-06-16 13:38:34.633770+00:00, queued_by_job_id=7, pid=41224[0m
[[34m2024-06-16T10:38:45.768-0300[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun indicium_challenge @ 2024-06-16 13:38:33.229324+00:00: manual__2024-06-16T13:38:33.229324+00:00, state:running, queued_at: 2024-06-16 13:38:33.726942+00:00. externally triggered: True> failed[0m
[[34m2024-06-16T10:38:45.771-0300[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=indicium_challenge, execution_date=2024-06-16 13:38:33.229324+00:00, run_id=manual__2024-06-16T13:38:33.229324+00:00, run_start_date=2024-06-16 13:38:34.508126+00:00, run_end_date=2024-06-16 13:38:45.769491+00:00, run_duration=11.261365, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-06-15 00:00:00+00:00, data_interval_end=2024-06-16 00:00:00+00:00, dag_hash=1e7b2027891a344c7b45d8f022ca3e8d[0m
[[34m2024-06-16T10:40:18.906-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T10:45:19.089-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T10:50:19.583-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T10:55:19.694-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T11:00:19.824-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T11:05:19.991-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T11:10:20.132-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T11:15:20.291-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T11:20:20.433-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T11:25:20.577-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T11:30:20.733-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T11:35:20.884-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T11:40:21.136-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T11:45:21.307-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T11:50:21.470-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T11:55:21.626-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T12:00:21.766-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T12:05:21.870-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T12:10:22.009-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T12:15:22.162-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T13:03:25.756-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T13:08:25.915-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T13:13:26.110-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T13:18:26.170-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T13:23:26.336-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T13:28:26.481-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T13:33:26.638-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T13:48:12.129-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T13:53:12.297-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T13:58:12.630-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T14:03:12.837-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T14:08:12.993-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T14:13:13.118-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T14:18:13.255-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T14:23:13.416-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T14:28:13.585-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T14:33:13.734-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T14:38:13.890-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T14:43:14.042-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T14:48:14.200-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T14:53:14.366-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T14:58:14.535-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T15:03:14.723-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T15:08:14.917-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T15:13:15.034-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T15:18:15.529-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T15:23:16.093-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T15:28:16.226-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T15:49:18.293-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T15:54:18.494-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T15:59:18.660-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T16:04:18.792-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T16:09:18.984-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T16:14:19.181-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T16:19:19.447-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T16:24:19.721-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T16:29:19.879-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T16:34:20.022-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T16:39:20.061-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T16:44:20.116-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T16:49:20.185-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T16:54:20.376-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T16:59:20.458-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T17:04:20.605-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T17:09:20.751-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T17:14:20.771-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T17:19:20.905-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T17:24:21.089-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T17:29:22.186-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T17:34:24.878-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T17:39:25.011-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T17:44:25.240-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T17:49:25.401-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T17:54:25.584-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T17:59:25.779-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T18:04:25.942-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T18:09:26.068-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T18:14:26.195-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T18:19:26.362-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T18:24:26.544-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T18:29:26.670-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T18:34:26.714-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T19:12:59.097-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T19:17:59.138-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T19:22:59.197-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T19:27:59.362-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T19:32:59.943-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T19:38:00.102-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T19:43:00.335-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T19:48:00.542-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T19:53:00.600-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T19:58:00.630-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T20:03:00.722-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T20:08:00.874-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T20:13:01.010-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T20:18:01.249-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T20:23:01.485-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T20:28:01.607-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T20:33:01.736-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T21:31:31.132-0300[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for indicium_challenge to 2024-06-17 00:00:00+00:00, run_after=2024-06-18 00:00:00+00:00[0m
[[34m2024-06-16T21:31:32.673-0300[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: indicium_challenge.extracts.csv scheduled__2024-06-16T00:00:00+00:00 [scheduled]>[0m
[[34m2024-06-16T21:31:32.677-0300[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG indicium_challenge has 0/16 running and queued tasks[0m
[[34m2024-06-16T21:31:32.681-0300[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: indicium_challenge.extracts.csv scheduled__2024-06-16T00:00:00+00:00 [scheduled]>[0m
[[34m2024-06-16T21:31:33.012-0300[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='indicium_challenge', task_id='extracts.csv', run_id='scheduled__2024-06-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-06-16T21:31:33.019-0300[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'indicium_challenge', 'extracts.csv', 'scheduled__2024-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T21:31:33.064-0300[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'indicium_challenge', 'extracts.csv', 'scheduled__2024-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py'][0m
[[34m2024-06-16T21:31:48.129-0300[0m] {[34mcli_parser.py:[0m77} WARNING[0m - cannot load CLI commands from auth manager: Type annotation for "TaskInstance.dag_model" can't be correctly interpreted for Annotated Declarative Table form.  ORM annotations should normally make use of the ``Mapped[]`` generic type, or other ORM-compatible generic type, as a container for the actual type, which indicates the intent that the attribute is mapped. Class variables that are not intended to be mapped by the ORM should use ClassVar[].  To allow Annotated Declarative to disregard legacy annotations which don't use Mapped[] to pass, set "__allow_unmapped__ = True" on the class or a superclass this class. (Background on this error at: https://sqlalche.me/e/20/zlpr)[0m
[[34m2024-06-16T21:31:48.135-0300[0m] {[34mcli_parser.py:[0m78} WARNING[0m - Authentication manager is not configured and webserver will not be able to start.[0m
[[34m2024-06-16T21:31:48.921-0300[0m] {[34msequential_executor.py:[0m81} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'indicium_challenge', 'extracts.csv', 'scheduled__2024-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_data.py']' returned non-zero exit status 1..[0m
[[34m2024-06-16T21:31:48.948-0300[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='indicium_challenge', task_id='extracts.csv', run_id='scheduled__2024-06-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-16T21:31:49.065-0300[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=indicium_challenge, task_id=extracts.csv, run_id=scheduled__2024-06-16T00:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=0, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=DockerOperator, queued_dttm=2024-06-17 00:31:32.687950+00:00, queued_by_job_id=7, pid=None[0m
[[34m2024-06-16T21:31:49.069-0300[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Executor reports task instance <TaskInstance: indicium_challenge.extracts.csv scheduled__2024-06-16T00:00:00+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T21:31:49.210-0300[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - Executor reports task instance <TaskInstance: indicium_challenge.extracts.csv scheduled__2024-06-16T00:00:00+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2024-06-16T21:31:49.275-0300[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=indicium_challenge, task_id=extracts.csv, run_id=scheduled__2024-06-16T00:00:00+00:00, execution_date=20240616T000000, start_date=, end_date=20240617T003149[0m
[[34m2024-06-16T21:31:53.857-0300[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun indicium_challenge @ 2024-06-16 00:00:00+00:00: scheduled__2024-06-16T00:00:00+00:00, state:running, queued_at: 2024-06-17 00:31:29.893610+00:00. externally triggered: False> failed[0m
[[34m2024-06-16T21:31:53.859-0300[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=indicium_challenge, execution_date=2024-06-16 00:00:00+00:00, run_id=scheduled__2024-06-16T00:00:00+00:00, run_start_date=2024-06-17 00:31:31.335975+00:00, run_end_date=2024-06-17 00:31:53.857687+00:00, run_duration=22.521712, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-16 00:00:00+00:00, data_interval_end=2024-06-17 00:00:00+00:00, dag_hash=1e7b2027891a344c7b45d8f022ca3e8d[0m
[[34m2024-06-16T21:31:53.868-0300[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for indicium_challenge to 2024-06-17 00:00:00+00:00, run_after=2024-06-18 00:00:00+00:00[0m
[[34m2024-06-16T21:34:08.665-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T21:39:09.040-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-16T21:44:09.340-0300[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
